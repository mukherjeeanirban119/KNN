{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The principle behind nearest neighbor methods is to find a predefined  number of training samples closest \n",
    "in distance to the new point, and predict the label from these.it is a a non-parametric method.In classification \n",
    "we find out the maximum classes under  predifine neaibours (k=33 ,3 ,4 etc) ,and in regression we find out the avarage \n",
    "distance  \n",
    "\n",
    "t can be used in superviced (regression,classification) and unsuperviced (k-nn clastering) learning  \n",
    "Despite its simplicity, nearest neighbors has been successful in a large number of classification and regression problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It is use for when N < 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.)Before fitting the model we should taake care of outlaiers and imbalance data set.\n",
    "\n",
    "2.)KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, \n",
    "the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance \n",
    "between the observations,and hence on the KNN classifier, than variables that are on a small scale.\n",
    "\n",
    "3.)Regarding the Nearest Neighbors algorithms, if two neighbors  and  have identical distances but different labels, \n",
    "the result will depend on the ordering of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Unsupervised Nearest Neighbors typically use to Finding the distance between Neighbors no succh application,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = nbrs.kneighbors(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.41421356],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.41421356]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances #the nearest neighbor of each point (ex [-1,-1]) is the point itself, at a distance of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbrs.kneighbors_graph(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [2, 1],\n",
       "       [3, 4],\n",
       "       [4, 3],\n",
       "       [5, 4]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " KNeighborsClassifier and  RadiusNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighborsClassifier\n",
    "the most commonly used technique. \n",
    "The optimal choice of the value  is highly data-dependent.\n",
    "Larger  suppresses the effects of noise, but makes the classification boundaries less distinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "knn.fit(X_train,y_train)\n",
    "pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RadiusNeighborsClassifier\n",
    "where the data is not uniformly sampled RadiusNeighborsClassifier can be a better choice.\n",
    "The user specifies a fixed radius , such that points in scattered neighborhoods use fewer nearest neighbors for the classification.\n",
    "For high-dimensional parameter spaces, this method becomes less effective due to the so-called “curse of dimensionality”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "neigh = RadiusNeighborsClassifier(radius=1.0)\n",
    "knn.fit(X_train,y_train)\n",
    "pred = neigh.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nearest Neighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neighbors-based regression can be used in cases where the data labels are continuous rather than discrete variables. \n",
    "The label assigned to a query point is computed based on the mean of the labels of its nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighborsRegressor and RadiusNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RadiusNeighborsRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsRegressor\n",
    "neig_r = RadiusNeighborsRegressor(radius=1.0)\n",
    "neig_r.fit(X_train,y_train)\n",
    "pred = neig_r.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighborsRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "K_neigh_r = KNeighborsRegressor(n_neighbors=2)\n",
    "K_neigh_r(X_train,y_train)\n",
    "pred = K_neigh_r.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(y_true, y_pred))\n",
    "print(mean_squared_error(y_true, y_pred)) #mse\n",
    "print(mean_squared_error(y_true, y_pred, squared=False)) #RSME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parameterss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Choosing a K Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighborsint, default=5\n",
    "Number of neighbors to use by default for kneighbors queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights{‘uniform’, ‘distance’} or callable, default=’uniform’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the impact of nearest neighbors on the test point should be more than  the  further awaay points,\n",
    "\n",
    "'uniform': uniform weights. All points in each neighborhood are weighted equally.\n",
    "\n",
    "'distance': weight points by the inverse of their distance.\n",
    "in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "distance increses weight decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm and leaf_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'brute' will use brute-force\n",
    "Efficient brute-force neighbors searches can be very competitive for small data samples.\n",
    "Brute force query time is unchanged by data structure.\n",
    "Brute force query time is largely unaffected by the value of K\n",
    "\n",
    "‘kd_tree’ will use KDTree\n",
    "KD trees partition data along Cartesian axes,\n",
    "This is a significant improvement over brute-force for large \n",
    "Though the KD tree approach is very fast for low-dimensional D<20  neighbors searches, \n",
    "It becomes inefficient as  grows very large: this is one manifestation of the so-called “curse of dimensionality”. \n",
    "    \n",
    "‘ball_tree’ will use BallTree\n",
    "ball trees partition data in a series of nesting hyper-spheres.\n",
    "This makes tree construction more costly than that of the KD tree, \n",
    "but results in a data structure which can be very efficient on highly structured data, even in very high dimensions.\n",
    "\n",
    "Ball tree and KD tree query times can be greatly influenced by data structure\n",
    "Ball tree and KD tree query time will become slower as K increases\n",
    "\n",
    "the ability to prune branches in a tree-based query is reduced. In this situation, Brute force queries can be more efficient.\n",
    "where K>N(sample)\n",
    "\n",
    "leaf_sizeint, default=30\n",
    "Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, \n",
    "as well as the memory required to store the tree.\n",
    "The optimal value depends on the nature of the problem.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric and p and metric_paramsdict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Euclidean /L2 norm /Ridge/ RMS distance : When p = 1, this is equivalent to using manhattan_distance (l1) \n",
    "it is written as sqrt(sum((x - y)^2)),do not preffer under the attact of outlaier and also high dimensanality (>5)\n",
    "\n",
    "Manhattan/ L1 norm/ Lasso/ MA(mean absolute) distance : euclidean_distance (l2) for p = 2, it is written as sum(|x - y|),\n",
    "do preffer under the attact of outlaier and also high dimensanality (>5)\n",
    "\n",
    "Minkowski :For arbitrary p (3,4,5 etc), minkowski_distance (l_p) is used,it is written as sum(|x - y|^p)^(1/p) ,\n",
    "find p by etaration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_paramsdict:Additional keyword arguments for the metric function ,\n",
    "it is used when CONDITION-1 is used to prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'euclidean' sqrt(sum((x - y)^2))\n",
    "\n",
    "'manhattan' sum(|x - y|)\n",
    "\n",
    "'chebyshev' max(|x - y|)\n",
    "\n",
    "'minkowski' sum(|x - y|^p)^(1/p)\n",
    "\n",
    "'wminkowski' sum(|w * (x - y)|^p)^(1/p)\n",
    "\n",
    "'seuclidean' sqrt(sum((x - y)^2 / V))\n",
    "\n",
    "'mahalanobis' sqrt((x - y)' V^-1 (x - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial import distance\n",
    "c=distance.canberra([1, 0, 0], [0, 1, 0])\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1,weights='distance',algorithm='brute', metric='mahalanobis')\n",
    "knn = KNeighborsClassifier(n_neighbors=1,weights='distance',algorithm='brute',metric_params='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[0], [1], [2], [3]]\n",
    "y = [0, 0, 1, 1]\n",
    "c=distance.canberra(X,2)\n",
    "neigh1 = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh2= KNeighborsClassifier(n_neighbors=2,weights='distance',algorithm='brute', metric='mahalanobis')\n",
    "neigh3= KNeighborsClassifier(n_neighbors=1,weights='distance',algorithm='brute',metric='seuclidean')\n",
    "X1 = [[1.3], [1], [0], [3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh1.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', metric='mahalanobis', n_neighbors=2,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh2.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', metric='seuclidean', n_neighbors=1,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh3.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1447: FutureWarning: from version 0.25, pairwise_distances for metric='seuclidean' will require V to be specified if Y is passed.\n",
      "  warnings.warn(\"from version 0.25, pairwise_distances for \"\n"
     ]
    }
   ],
   "source": [
    "pred1 = neigh3.predict(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', metric='mahalanobis', n_neighbors=2,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh1.fit(X, y)\n",
    "neigh2.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONDITION-1\n",
    "algorithm = 'auto' selects 'brute' if any of the following conditions are verified:\n",
    "\n",
    "input data is sparse\n",
    "\n",
    "metric = 'precomputed'\n",
    "\n",
    "D>15\n",
    "\n",
    "K>=N/2 find by itearation \n",
    "\n",
    "effective_metric_ isn’t in the VALID_METRICS list for either 'kd_tree' or 'ball_tree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it selects the first out of 'kd_tree' and 'ball_tree' that has effective_metric_ in its VALID_METRICS list. \n",
    "It is based on the following assumptions:\n",
    "\n",
    "the number of query points /test points is at least the same order as the number of training points\n",
    "\n",
    "leaf_size is close to its default value of 30\n",
    "\n",
    "when D>15 the intrinsic dimensionality of the data is generally too high for tree-based methods\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The actual performance is highly dependent on the structure of the training data.\n",
    "Datasets used in machine learning tend to be very structured, and are very well-suited for tree-based queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Choosing a K Value\n",
    "n_neighborsint, default=5\n",
    "Number of neighbors to use by default for kneighbors queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracy_rate = []\n",
    "# Will take some time\n",
    "for i in range(1,40):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    score=cross_val_score(knn,df_feat,df['TARGET CLASS'],cv=10)\n",
    "    accuracy_rate.append(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "#plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    "  #       markerfacecolor='red', markersize=10)\n",
    "plt.plot(range(1,40),accuracy_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('accuracy_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regressor (use explained_variance_score ,RSME OR R2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import explained_variance_score\n",
    "RSME = []\n",
    "# Will take some time\n",
    "for i in range(1,40):\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    score=mean_squared_error(df_feat,df['TARGET CLASS'])\n",
    "    RSME.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "#plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    "  #       markerfacecolor='red', markersize=10)\n",
    "plt.plot(range(1,40),RSME,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('RSME vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('RSME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
